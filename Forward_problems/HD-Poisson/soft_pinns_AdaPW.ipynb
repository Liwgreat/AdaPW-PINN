{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-28T06:12:03.497953Z",
     "start_time": "2024-12-28T06:12:01.991170Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "%matplotlib inline\n",
    "from torch import nn, optim, autograd\n",
    "from torch.nn import functional as F\n",
    "from pyDOE import lhs\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import matplotlib.gridspec as gridspec\n",
    "%matplotlib inline\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "from utils_training import *\n",
    "\n",
    "random_seed = 1234\n",
    "np.random.seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-28T06:12:03.525674Z",
     "start_time": "2024-12-28T06:12:03.500740Z"
    }
   },
   "outputs": [],
   "source": [
    "N_train = 10000\n",
    "N_bound = 500\n",
    "\n",
    "train_data = lhs(10,N_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-28T06:12:09.495685Z",
     "start_time": "2024-12-28T06:12:03.528881Z"
    }
   },
   "outputs": [],
   "source": [
    "bound_x_0_list = []\n",
    "for i in range(10):\n",
    "    bound_x_temp = lhs(10, N_bound)\n",
    "    bound_x_temp[:, i:i+1] = np.zeros((N_bound, 1))\n",
    "    bound_x_0_list.append(bound_x_temp)\n",
    "\n",
    "bound_x_0 = np.concatenate(bound_x_0_list, axis=0)\n",
    "\n",
    "bound_x_1_list = []\n",
    "for i in range(10):\n",
    "    bound_x_temp = lhs(10, N_bound)\n",
    "    bound_x_temp[:, i:i+1] = np.ones((N_bound, 1))\n",
    "    bound_x_1_list.append(bound_x_temp)\n",
    "\n",
    "bound_x_1 = np.concatenate(bound_x_1_list, axis=0)\n",
    "\n",
    "bound_x_data = np.concatenate((bound_x_0, bound_x_1), axis=0)\n",
    "\n",
    "bound_x_data = numpy_to_tensor(bound_x_data, var_name=\"bound_x_data\", value_range_dim = True, to_torch = True, to_cuda = True, requires_grad = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exact u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-28T06:12:09.505997Z",
     "start_time": "2024-12-28T06:12:09.498940Z"
    }
   },
   "outputs": [],
   "source": [
    "def u_solution(data_input):\n",
    "    \n",
    "    x1 = data_input[:, 0:1]\n",
    "    x2 = data_input[:, 1:2]\n",
    "    x3 = data_input[:, 2:3]\n",
    "    x4 = data_input[:, 3:4]\n",
    "    x5 = data_input[:, 4:5]\n",
    "    x6 = data_input[:, 5:6]\n",
    "    x7 = data_input[:, 6:7]\n",
    "    x8 = data_input[:, 7:8]\n",
    "    x9 = data_input[:, 8:9]\n",
    "    x10 = data_input[:, 9:10]\n",
    "    \n",
    "    u_value = x1**2-x2**2+x3**2-x4**2+x5*x6+x7*x8*x9*x10\n",
    "    \n",
    "    return u_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-28T06:12:09.538787Z",
     "start_time": "2024-12-28T06:12:09.508865Z"
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(5678)\n",
    "test_data = lhs(10,N_train)\n",
    "test_u = u_solution(test_data)\n",
    "\n",
    "test_data = numpy_to_tensor(test_data, var_name=\"test_data\", value_range_dim = True, to_torch = True, to_cuda = True, requires_grad = False)\n",
    "test_u = numpy_to_tensor(test_u, var_name=\"test_u\", value_range_dim = True, to_torch = True, to_cuda = True, requires_grad = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-28T06:12:09.554002Z",
     "start_time": "2024-12-28T06:12:09.541145Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_loss_f(train_data_input, PINNs, return_sequence='not'):\n",
    "    \n",
    "    # Extract f_data into individual variables (x1, x2, ..., x10)\n",
    "    x1_inside = train_data_input[:, 0:1]\n",
    "    x2_inside = train_data_input[:, 1:2]\n",
    "    x3_inside = train_data_input[:, 2:3]\n",
    "    x4_inside = train_data_input[:, 3:4]\n",
    "    x5_inside = train_data_input[:, 4:5]\n",
    "    x6_inside = train_data_input[:, 5:6]\n",
    "    x7_inside = train_data_input[:, 6:7]\n",
    "    x8_inside = train_data_input[:, 7:8]\n",
    "    x9_inside = train_data_input[:, 8:9]\n",
    "    x10_inside = train_data_input[:, 9:10]\n",
    "\n",
    "    x1_inside = numpy_to_tensor(x1_inside, var_name=\"x1_inside\", value_range_dim = False, to_torch = True, to_cuda = True, requires_grad = True)\n",
    "    x2_inside = numpy_to_tensor(x2_inside, var_name=\"x2_inside\", value_range_dim = False, to_torch = True, to_cuda = True, requires_grad = True)\n",
    "    x3_inside = numpy_to_tensor(x3_inside, var_name=\"x3_inside\", value_range_dim = False, to_torch = True, to_cuda = True, requires_grad = True)\n",
    "    x4_inside = numpy_to_tensor(x4_inside, var_name=\"x4_inside\", value_range_dim = False, to_torch = True, to_cuda = True, requires_grad = True)\n",
    "    x5_inside = numpy_to_tensor(x5_inside, var_name=\"x5_inside\", value_range_dim = False, to_torch = True, to_cuda = True, requires_grad = True)\n",
    "    x6_inside = numpy_to_tensor(x6_inside, var_name=\"x6_inside\", value_range_dim = False, to_torch = True, to_cuda = True, requires_grad = True)\n",
    "    x7_inside = numpy_to_tensor(x7_inside, var_name=\"x7_inside\", value_range_dim = False, to_torch = True, to_cuda = True, requires_grad = True)\n",
    "    x8_inside = numpy_to_tensor(x8_inside, var_name=\"x8_inside\", value_range_dim = False, to_torch = True, to_cuda = True, requires_grad = True)\n",
    "    x9_inside = numpy_to_tensor(x9_inside, var_name=\"x9_inside\", value_range_dim = False, to_torch = True, to_cuda = True, requires_grad = True)\n",
    "    x10_inside = numpy_to_tensor(x10_inside, var_name=\"x10_inside\", value_range_dim = False, to_torch = True, to_cuda = True, requires_grad = True) \n",
    "    \n",
    "    E_inside = PINNs1(torch.cat((x1_inside,x2_inside,x3_inside,x4_inside,x5_inside,x6_inside,x7_inside,x8_inside,x9_inside,x10_inside),1))\n",
    "    \n",
    "    E_xx1 = compute_higher_order_derivatives(E_inside, [x1_inside,x1_inside])\n",
    "    E_xx2 = compute_higher_order_derivatives(E_inside, [x2_inside,x2_inside])\n",
    "    E_xx3 = compute_higher_order_derivatives(E_inside, [x3_inside,x3_inside])\n",
    "    E_xx4 = compute_higher_order_derivatives(E_inside, [x4_inside,x4_inside])\n",
    "    E_xx5 = compute_higher_order_derivatives(E_inside, [x5_inside,x5_inside])\n",
    "    E_xx6 = compute_higher_order_derivatives(E_inside, [x6_inside,x6_inside])\n",
    "    E_xx7 = compute_higher_order_derivatives(E_inside, [x7_inside,x7_inside])\n",
    "    E_xx8 = compute_higher_order_derivatives(E_inside, [x8_inside,x8_inside])\n",
    "    E_xx9 = compute_higher_order_derivatives(E_inside, [x9_inside,x9_inside])\n",
    "    E_xx10 = compute_higher_order_derivatives(E_inside, [x10_inside,x10_inside])\n",
    "    \n",
    "    loss_term = E_xx1+E_xx2+E_xx3+E_xx4+E_xx5+E_xx6+E_xx7+E_xx8+E_xx9+E_xx10\n",
    "    \n",
    "    if return_sequence == 'yes':\n",
    "        return torch.square(loss_term)\n",
    "    else:\n",
    "        return torch.mean(torch.square(loss_term))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PINN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-28T06:12:09.558979Z",
     "start_time": "2024-12-28T06:12:09.555668Z"
    }
   },
   "outputs": [],
   "source": [
    "#Paper reproduction\n",
    "torch.manual_seed(1234)\n",
    "torch.cuda.manual_seed(1234)\n",
    "np.random.seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-28T06:12:09.594103Z",
     "start_time": "2024-12-28T06:12:09.560974Z"
    }
   },
   "outputs": [],
   "source": [
    "net_settings_for_PINNs1 = NetSetting(input_dims=10, hidden_neurons_list=[100]*5, \n",
    "                                     output_dims=1, hidden_activation='tanh', \n",
    "                                     output_activation=None, initializer_method='xavier')\n",
    "PINNs1 = get_mlp_pinn(net_settings_for_PINNs1)\n",
    "PINNs1.cuda()\n",
    "\n",
    "initialize_weights(PINNs1, net_settings_for_PINNs1.initializer_method)\n",
    "\n",
    "optimizer1 = optim.Adam([{'params': PINNs1.parameters()}], lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-28T06:12:09.602528Z",
     "start_time": "2024-12-28T06:12:09.598162Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_mini_batches(train_data_new, batch_size):\n",
    "    \n",
    "    indices = np.arange(len(train_data_new))\n",
    "    np.random.shuffle(indices)\n",
    "    \n",
    "    for start_idx in range(0, len(train_data_new) - batch_size + 1, batch_size):\n",
    "        \n",
    "        excerpt_batch = indices[start_idx:start_idx + batch_size]\n",
    "        train_data_batch = train_data_new[excerpt_batch,:]\n",
    "\n",
    "        yield train_data_batch, excerpt_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-28T06:12:09.836574Z",
     "start_time": "2024-12-28T06:12:09.604816Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "def update_collocation_points_PW(Number_select, Number_all, train_data_old, weight_sequence_old):\n",
    "\n",
    "    train_data = lhs(10,Number_all)\n",
    "    \n",
    "    random_indices_perm = torch.randperm(Number_all)\n",
    "    random_indices = random_indices_perm[:Number_select]\n",
    "    \n",
    "    train_data_new = train_data[random_indices,:]\n",
    "\n",
    "    nbrs = NearestNeighbors(n_neighbors=1, algorithm='auto').fit(train_data_old)\n",
    "      \n",
    "    distances, nearest_indices = nbrs.kneighbors(train_data_new)\n",
    "    weight_sequence_new = weight_sequence_old[nearest_indices.reshape(-1,)]\n",
    "\n",
    "    return train_data_new,weight_sequence_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-28T06:12:09.845290Z",
     "start_time": "2024-12-28T06:12:09.839362Z"
    }
   },
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "    print(\"Using CUDA on GPU\")\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "    print(\"Using CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-28T06:12:09.851529Z",
     "start_time": "2024-12-28T06:12:09.847381Z"
    }
   },
   "outputs": [],
   "source": [
    "##############PW_L_f###############\n",
    "e_k = 5e-4\n",
    "q_k = 1e-4\n",
    "s_k = 0.5\n",
    "Ada_CL_L_f = Ada_CL(e_k,q_k,s_k,device)\n",
    "weight_sequence_L_f = (1/N_train)*np.ones((N_train,1))\n",
    "##############PW_L_f###############   \n",
    "\n",
    "loss_all_1 = []\n",
    "loss_all_2 = []\n",
    "loss_f_1 = []\n",
    "loss_f_2 = []\n",
    "loss_b_1 = []\n",
    "test_loss_1 = []\n",
    "beta_number_list = []\n",
    "rho_k_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-28T06:32:50.645364Z",
     "start_time": "2024-12-28T06:12:09.853841Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nIter2 = 1000\n",
    "it = 0\n",
    "Number_select = 10000\n",
    "Number_all = 10000\n",
    "batch_size = 1000\n",
    "\n",
    "while it<nIter2 :\n",
    "    \n",
    "    if it==0:\n",
    "        ##############PW_L_f###############\n",
    "        loss_f_sequence = get_loss_f(train_data, PINNs1, return_sequence='yes')\n",
    "        loss_f,weight_sequence_L_f,rho_k,beta_sequence = Ada_CL_L_f.Training_Scheduler_torch(loss_f_sequence, \n",
    "                                                                                             weight_sequence_L_f,\n",
    "                                                                                             alpha_k_function = 'default',\n",
    "                                                                                             record = 'yes')\n",
    "        beta_number_list.append(np.sum(beta_sequence)/N_train)\n",
    "        rho_k_list.append(rho_k)\n",
    "        ##############PW_L_f###############  \n",
    "    \n",
    "    else:\n",
    "        train_data, weight_sequence_new = update_collocation_points_PW(Number_select, Number_all, train_data, weight_sequence_L_f)\n",
    "        weight_sequence_L_f = weight_sequence_new/np.sum(weight_sequence_new)\n",
    "    \n",
    "    for train_data_batch, batch_indices in create_mini_batches(train_data, batch_size):\n",
    "        \n",
    "        ##### loss bound ######\n",
    "        E_bound = PINNs1(bound_x_data)\n",
    "        real_bound = u_solution(bound_x_data)\n",
    "        loss_bound = torch.mean(torch.square(E_bound-real_bound))\n",
    "\n",
    "        ##############PW_L_f###############\n",
    "        loss_f_sequence = get_loss_f(train_data_batch, PINNs1, return_sequence='yes')\n",
    "        weight_sequence_L_f_tensor = torch.from_numpy(weight_sequence_L_f[batch_indices]).float()\n",
    "        loss_f  = torch.sum(loss_f_sequence*weight_sequence_L_f_tensor.cuda())\n",
    "        ##############PW_L_f###############   \n",
    "\n",
    "        #####loss PI#######\n",
    "        loss = loss_bound+loss_f \n",
    "  \n",
    "        optimizer1.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer1.step()\n",
    "        \n",
    "    #########  test_loss NRMSE  #########\n",
    "    pre_u = PINNs1(test_data)  \n",
    "    test_loss = relative_l2_torch(pre_u,test_u)\n",
    "    test_loss_1.append(test_loss)\n",
    "    #########  test_loss NRMSE  #########\n",
    "\n",
    "    ##############Record###############\n",
    "    loss_all_2.append(loss.item())\n",
    "    loss_f_2.append(loss_f.item())\n",
    "    loss_b_1.append(loss_bound.item())\n",
    "    ##############Record############### \n",
    "    \n",
    "    ##############Record Original###############\n",
    "    loss_f_original = get_loss_f(train_data_batch, PINNs1, return_sequence='not')\n",
    "    loss_all_original = loss_bound+loss_f_original\n",
    "    loss_all_1.append(loss_all_original.item())\n",
    "    loss_f_1.append(loss_f_original.item())                 \n",
    "    ##############Record Original###############\n",
    "    \n",
    "    ##############PW_L_f###############\n",
    "    loss_f_sequence = get_loss_f(train_data, PINNs1, return_sequence='yes')\n",
    "    loss_f,weight_sequence_L_f,rho_k,beta_sequence = Ada_CL_L_f.Training_Scheduler_torch(loss_f_sequence, \n",
    "                                                                                         weight_sequence_L_f,\n",
    "                                                                                         alpha_k_function = 'default',\n",
    "                                                                                         record = 'yes')\n",
    "    beta_number_list.append(np.sum(beta_sequence)/N_train)\n",
    "    rho_k_list.append(rho_k)\n",
    "    ##############PW_L_f###############  \n",
    "    \n",
    "    if it % 100 == 0:\n",
    "        print(f'It: {it}, train_loss: {loss.item()}, test_loss: {test_loss}, best_test_loss: {min(test_loss_1):.4f}')\n",
    "    it = it + 1        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GA-PINNs",
   "language": "python",
   "name": "gapings"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
